{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d51c4d62",
   "metadata": {},
   "source": [
    "# Дообучение модели для вопросно-ответной задачи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d660502b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/.pyenv/versions/torch/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    pipeline,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    default_data_collator,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8105fad9",
   "metadata": {},
   "source": [
    "Дообучим модель *distilroberta-base* на данных *SQuAD* для *задачи извлечения ответов из текста (Question Answering)*.\n",
    "\n",
    "*distilroberta-base* - это уменьшенная и оптимизированная версия модели RoBERTa, сохраняющая высокое качество, но работающая быстрее.\n",
    "\n",
    "*SQuAD* - популярный датасет для оценки производительности моделей в задаче вопросно-ответных систем, содержащий вопросы и ответы на основе википедийных статей.\n",
    "\n",
    "Что нужно сделать:\n",
    "1. Выбрать предобученную модель\n",
    "2. Загрузить соответствующий токенизатор для выбранной модели\n",
    "3. Разметить и векторизовать данные\n",
    "4. Загрузить предобученную модель\n",
    "5. Дообучить на новых данных под свою задачу."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65200735",
   "metadata": {},
   "source": [
    "## Подготовка последовательностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dd95e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0949bb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 87599\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 10570\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "762f6313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One of the main driving forces in the growth o...</td>\n",
       "      <td>In what year did the team lead by Knute Rockne...</td>\n",
       "      <td>{'text': ['1925'], 'answer_start': [354]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the main driving forces in the growth o...</td>\n",
       "      <td>How many years was Knute Rockne head coach at ...</td>\n",
       "      <td>{'text': ['13'], 'answer_start': [251]}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>One of the main driving forces in the growth o...</td>\n",
       "      <td>How many national titles were won when Knute R...</td>\n",
       "      <td>{'text': ['three'], 'answer_start': [274]}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  One of the main driving forces in the growth o...   \n",
       "4  One of the main driving forces in the growth o...   \n",
       "5  One of the main driving forces in the growth o...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3  In what year did the team lead by Knute Rockne...   \n",
       "4  How many years was Knute Rockne head coach at ...   \n",
       "5  How many national titles were won when Knute R...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3          {'text': ['1925'], 'answer_start': [354]}  \n",
       "4            {'text': ['13'], 'answer_start': [251]}  \n",
       "5         {'text': ['three'], 'answer_start': [274]}  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    data[\"train\"][0, 1, 2, 100, 101, 102],\n",
    "    columns=[\"context\", \"question\", \"answers\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f70409cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilroberta-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c59b49c3",
   "metadata": {},
   "source": [
    "Кодирование одной строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c8d20c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 13841, 64, 38, 465, 10, 26432, 6971, 116, 2]\n"
     ]
    }
   ],
   "source": [
    "t = \"Where can I find a pizzeria?\"\n",
    "print(tokenizer.encode(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04457952",
   "metadata": {},
   "source": [
    "Кодирование батча. Так как есть только один образец, то `attention_mask` заполнен `1` (не дополнен нулями (not padded))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62a12730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 13841, 64, 38, 465, 10, 26432, 6971, 116, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_t = tokenizer(t)\n",
    "print(encoded_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62cff04",
   "metadata": {},
   "source": [
    "`convert_ids_to_tokens` из массива индексов получает токены. Токенизатор `distilrobera-base` в начале и в конце последовательности ставит специальные токены начала и конца (`<s>` и `</s>`). Также ставит символ `Ġ` перед целыми словом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6293106f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Where', 'Ġcan', 'ĠI', 'Ġfind', 'Ġa', 'Ġpizz', 'eria', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded_t[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303c024",
   "metadata": {},
   "source": [
    "Нам нужно кодировать и вопрос и ответ как пару. Можем строки передавать попарна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9024d9b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [0, 9226, 16, 10, 864, 2, 2, 9226, 16, 5, 5377, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_pair = tokenizer(\"this is a question\", \"this is the context\")\n",
    "print(encoded_pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc8ba828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'this', 'Ġis', 'Ġa', 'Ġquestion', '</s>', '</s>', 'this', 'Ġis', 'Ġthe', 'Ġcontext', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(encoded_pair[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7be86e5",
   "metadata": {},
   "source": [
    "Есть две версии токенизаторов: 1) реализованный на Python и 2) реализованный на Rust (быстрее)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e66d469f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91934e7",
   "metadata": {},
   "source": [
    "*Roberta* имеет ограничение длины последовательности в *512* токенов. Мы можем настраивать этот параметр."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b028efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"Sarah went to The Mirthless Cafe last night to meet her friend.\"\n",
    "question = \"Where did Sarah go?\"\n",
    "\n",
    "# The answer span and the answer's starting character position in the context.\n",
    "answer = \"The Mirthless Cafe\"\n",
    "answer_start = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2108dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 1672, 16542, 94, 363, 7, 972, 69, 1441, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(question, context)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "effe63e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Where',\n",
       " ' did',\n",
       " ' Sarah',\n",
       " ' go',\n",
       " '?',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Sarah',\n",
       " ' went',\n",
       " ' to',\n",
       " ' The',\n",
       " ' M',\n",
       " 'irth',\n",
       " 'less',\n",
       " ' Cafe',\n",
       " ' last',\n",
       " ' night',\n",
       " ' to',\n",
       " ' meet',\n",
       " ' her',\n",
       " ' friend',\n",
       " '.',\n",
       " '</s>']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c013c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_max_length = 15\n",
    "x = tokenizer(\n",
    "    question, context, max_length=example_max_length, truncation=\"only_second\"\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b38194",
   "metadata": {},
   "source": [
    "Проблема в том, что ответ может обрезаться или вовсе не включаться в последовательность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a1ca894e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>',\n",
       " 'Where',\n",
       " ' did',\n",
       " ' Sarah',\n",
       " ' go',\n",
       " '?',\n",
       " '</s>',\n",
       " '</s>',\n",
       " 'Sarah',\n",
       " ' went',\n",
       " ' to',\n",
       " ' The',\n",
       " ' M',\n",
       " 'irth',\n",
       " '</s>']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb18b0c",
   "metadata": {},
   "source": [
    "Чтобы гарантировать токенизацию всех токенов контекста с соблюдением максимальной длины, мы можем установить для параметра *return_overflowing_tokens* значение True. Конечный эффект заключается в разделении входных данных на несколько пар \"вопрос/контекст\", где каждая последующая последовательность контекста является продолжением предыдущей. Поскольку последняя из них может быть короче максимальной длины, мы также устанавливаем длину заполнения справа (padding).\n",
    "\n",
    "То, что мы получаем в ответ, - это несколько последовательностей *input_id*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d1283cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 1672, 16542, 94, 363, 7, 972, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 69, 1441, 4, 2, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0]], 'overflow_to_sample_mapping': [0, 0, 0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=example_max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "122a69ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f36163ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Where did Sarah go?</s></s>Sarah went to The Mirth</s>',\n",
       " '<s>Where did Sarah go?</s></s>less Cafe last night to meet</s>',\n",
       " '<s>Where did Sarah go?</s></s> her friend.</s><pad><pad><pad>']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7e8e85b",
   "metadata": {},
   "source": [
    "- В последней последовательности attention_mask присутствуют нули, обозначающие заполнение (padding).\n",
    "- Массив overflow_to_sample_mapping показывает, из какой пары \"вопрос/контекст\" произошла каждая последовательность input_ids. В нашем примере мы токенизировали одну пару \"вопрос/контекст\", что привело к созданию трёх последовательностей input_ids, поэтому overflow_to_sample_mapping состоит из трёх нулей.\n",
    "- Если бы мы токенизировали две пары \"вопрос/контекст\", мы бы увидели, что overflow_to_sample_mapping отражает это."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c967ada",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 40018, 112, 2, 2, 46796, 112, 2], [0, 40018, 132, 2, 2, 46796, 132, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\n",
    "    [\"question 1\", \"question 2\"],\n",
    "    [\"context 1\", \"context 2\"],\n",
    "    return_overflowing_tokens=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d163690e",
   "metadata": {},
   "source": [
    "Однако здесь всё ещё остаётся проблема, заключающаяся в том, что ни одна из последовательностей не содержит полный ответ (\"The Mirthless Cafe\"). В данном случае правильный полный ответ разделён между последовательностями.\n",
    "\n",
    "Чтобы устранить это, мы можем токенизировать наши пары \"вопрос/контекст\" в перекрывающиеся последовательности, установив длину шага (stride)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa9a6a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stride = 5\n",
    "x = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=example_max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=stride,\n",
    "    padding=\"max_length\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082cc87e",
   "metadata": {},
   "source": [
    "Установив шаг (stride), равный 5, каждая последующая последовательность контекста начинается на 5 подслов раньше относительно предыдущей последовательности.\n",
    "\n",
    "Таким образом, две из наших токенизированных последовательностей теперь содержат полный ответ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10c0f04a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Where did Sarah go?</s></s>Sarah went to The Mirth</s>',\n",
       " '<s>Where did Sarah go?</s></s> went to The Mirthless</s>',\n",
       " '<s>Where did Sarah go?</s></s> to The Mirthless Cafe</s>',\n",
       " '<s>Where did Sarah go?</s></s> The Mirthless Cafe last</s>',\n",
       " '<s>Where did Sarah go?</s></s> Mirthless Cafe last night</s>',\n",
       " '<s>Where did Sarah go?</s></s>irthless Cafe last night to</s>',\n",
       " '<s>Where did Sarah go?</s></s>less Cafe last night to meet</s>',\n",
       " '<s>Where did Sarah go?</s></s> Cafe last night to meet her</s>',\n",
       " '<s>Where did Sarah go?</s></s> last night to meet her friend</s>',\n",
       " '<s>Where did Sarah go?</s></s> night to meet her friend.</s>']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd7f1e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeysView({'input_ids': [[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 439, 7, 20, 256, 24208, 1672, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 7, 20, 256, 24208, 1672, 16542, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 20, 256, 24208, 1672, 16542, 94, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 256, 24208, 1672, 16542, 94, 363, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 24208, 1672, 16542, 94, 363, 7, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 1672, 16542, 94, 363, 7, 972, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 16542, 94, 363, 7, 972, 69, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 94, 363, 7, 972, 69, 1441, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 363, 7, 972, 69, 1441, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 439, 7, 20, 256, 24208, 1672, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 7, 20, 256, 24208, 1672, 16542, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 20, 256, 24208, 1672, 16542, 94, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 256, 24208, 1672, 16542, 94, 363, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 24208, 1672, 16542, 94, 363, 7, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 1672, 16542, 94, 363, 7, 972, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 16542, 94, 363, 7, 972, 69, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 94, 363, 7, 972, 69, 1441, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 363, 7, 972, 69, 1441, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x.keys(), \"\\n\")\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340eb4de",
   "metadata": {},
   "source": [
    "Для тонкой настройки модели для ответов на вопросы наша предварительно обученная модель *distilroberta-base* ожидает, что этот объект будет содержать ещё и:\n",
    "\n",
    "- *start_positions*: позиции токенов, где начинаются ответы.\n",
    "- *end_positions*: позиции токенов, где заканчиваются ответы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e13a425d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "The Mirthless Cafe\n"
     ]
    }
   ],
   "source": [
    "print(answer_start)\n",
    "print(context[answer_start : answer_start + len(answer)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476ed7c8",
   "metadata": {},
   "source": [
    "Нам нужно использовать это, чтобы определить позиции <u>токенов</u>, где каждый ответ начинается и заканчивается в каждой последовательности input_ids. В некоторых случаях полный ответ может отсутствовать в конкретной последовательности. Нам также нужно обрабатывать такие ситуации.\n",
    "\n",
    "Для этого мы получим дополнительную информацию, установив для параметра return_offsets_mapping значение True в токенизаторе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3558824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 439, 7, 20, 256, 24208, 1672, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 7, 20, 256, 24208, 1672, 16542, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 20, 256, 24208, 1672, 16542, 94, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 256, 24208, 1672, 16542, 94, 363, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 24208, 1672, 16542, 94, 363, 7, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 1672, 16542, 94, 363, 7, 972, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 16542, 94, 363, 7, 972, 69, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 94, 363, 7, 972, 69, 1441, 2], [0, 13841, 222, 4143, 213, 116, 2, 2, 363, 7, 972, 69, 1441, 4, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'offset_mapping': [[(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (0, 5), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23), (23, 27), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (11, 13), (14, 17), (18, 19), (19, 23), (23, 27), (28, 32), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (14, 17), (18, 19), (19, 23), (23, 27), (28, 32), (33, 37), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (18, 19), (19, 23), (23, 27), (28, 32), (33, 37), (38, 43), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (19, 23), (23, 27), (28, 32), (33, 37), (38, 43), (44, 46), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (23, 27), (28, 32), (33, 37), (38, 43), (44, 46), (47, 51), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (28, 32), (33, 37), (38, 43), (44, 46), (47, 51), (52, 55), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (33, 37), (38, 43), (44, 46), (47, 51), (52, 55), (56, 62), (0, 0)], [(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (38, 43), (44, 46), (47, 51), (52, 55), (56, 62), (62, 63), (0, 0)]], 'overflow_to_sample_mapping': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tokenizer(\n",
    "    question,\n",
    "    context,\n",
    "    max_length=example_max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    stride=stride,\n",
    "    return_offsets_mapping=True,\n",
    "    padding=\"max_length\",\n",
    ")\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c5405f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(x[\"input_ids\"]))\n",
    "print(len(x[\"offset_mapping\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65aa6f01",
   "metadata": {},
   "source": [
    "Каждый элемент в offset_mapping указывает начальную и конечную позицию символа для каждого токена в исходной строке. Сопоставление смещения (0, 0) представляет специальный токен (например, `<s>`).\n",
    "\n",
    "Например, вот первая последовательность input_ids вместе с её соответствующим offset_mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "edbe63b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2]\n",
      "[(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (0, 5), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(x[\"input_ids\"][0])\n",
    "print(x[\"offset_mapping\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "86c6b131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First non-special input_id converted to token:\n",
      "Where \n",
      "\n",
      "Span extracted from context using corresponding offset_mapping (0, 5):\n",
      "Where\n"
     ]
    }
   ],
   "source": [
    "print(\"First non-special input_id converted to token:\")\n",
    "print(tokenizer.convert_ids_to_tokens(x[\"input_ids\"][0][1]), \"\\n\")\n",
    "\n",
    "offset = x[\"offset_mapping\"][0][1]\n",
    "print(\n",
    "    f\"Span extracted from context using corresponding offset_mapping {offset}:\"\n",
    ")\n",
    "print(question[offset[0] : offset[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d4ab8",
   "metadata": {},
   "source": [
    "Поскольку нам известна позиция символа, с которой начинается ответ, мы можем использовать её вместе с *offset_mapping*, чтобы определить начальную и конечную позиции токенов для отрезка ответа.\n",
    "\n",
    "Единственная оставшаяся проблема — это определение того, относится ли смещение к вопросу или к контексту. Если посмотреть на первые два *offset_mapping*, можно заметить, что:\n",
    "\n",
    "1.  В первой последовательности оба *offset_mapping* (как для вопроса, так и для контекста) начинаются с нуля.\n",
    "2.  Во второй последовательности значения *offset_mapping* для контекста продолжаются с предыдущей последовательности (с учётом заданного шага)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "005134f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (0, 5), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23), (0, 0)]\n",
      "[(0, 0), (0, 5), (6, 9), (10, 15), (16, 18), (18, 19), (0, 0), (0, 0), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23), (23, 27), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "print(x[\"offset_mapping\"][0])\n",
    "print(x[\"offset_mapping\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d74422d",
   "metadata": {},
   "source": [
    "Это означает, что нам необходимо определить:\n",
    "\n",
    "1. Какие из *offset_mapping* относятся к контексту.\n",
    "2. Содержит ли конкретная последовательность ответ вообще.\n",
    "\n",
    "Первая задача может быть выполнена с помощью метода *sequence_ids* для объекта кодирования. Каждая последовательность *input_ids* имеет соответствующий список *sequence_ids*, который указывает, является ли токен частью вопроса, частью контекста или специальным токеном."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "184d2688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 13841, 222, 4143, 213, 116, 2, 2, 33671, 439, 7, 20, 256, 24208, 2]\n",
      "[None, 0, 0, 0, 0, 0, None, None, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "print(x[\"input_ids\"][0])\n",
    "print(x.sequence_ids(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98ff96a",
   "metadata": {},
   "source": [
    "Таким образом, чтобы определить, является ли токен частью контекста, мы можем использовать sequence_ids и проверить, соответствует ли позиция токена значению 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f53d7e",
   "metadata": {},
   "source": [
    "Для решения второй проблемы мы можем проверить, находятся ли начальная и конечная позиции символов ответа в пределах наименьшего и наибольшего значений сопоставления смещений (offset mapping) соответственно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59ed82a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer start character position: 14\n",
      "Answer end character position: 32\n",
      "Answer pulled from context: The Mirthless Cafe\n"
     ]
    }
   ],
   "source": [
    "# We can calculate the answer end character position using the answer length.\n",
    "answer_end = answer_start + len(answer)\n",
    "\n",
    "print(\"Answer start character position:\", answer_start)\n",
    "print(\"Answer end character position:\", answer_end)\n",
    "print(\"Answer pulled from context:\", context[answer_start:answer_end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f324de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s>Where did Sarah go?</s></s>Sarah went to The Mirth</s>',\n",
       " '<s>Where did Sarah go?</s></s> went to The Mirthless</s>',\n",
       " '<s>Where did Sarah go?</s></s> to The Mirthless Cafe</s>',\n",
       " '<s>Where did Sarah go?</s></s> The Mirthless Cafe last</s>',\n",
       " '<s>Where did Sarah go?</s></s> Mirthless Cafe last night</s>',\n",
       " '<s>Where did Sarah go?</s></s>irthless Cafe last night to</s>',\n",
       " '<s>Where did Sarah go?</s></s>less Cafe last night to meet</s>',\n",
       " '<s>Where did Sarah go?</s></s> Cafe last night to meet her</s>',\n",
       " '<s>Where did Sarah go?</s></s> last night to meet her friend</s>',\n",
       " '<s>Where did Sarah go?</s></s> night to meet her friend.</s>']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_decode(x[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e76f4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = x[\"input_ids\"][0]\n",
    "offset_mapping = x[\"offset_mapping\"][0]\n",
    "seq_ids = x.sequence_ids(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02aa171a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence IDs:  [None, 0, 0, 0, 0, 0, None, None, 1, 1, 1, 1, 1, 1, None]\n"
     ]
    }
   ],
   "source": [
    "# These are the sequence ids\n",
    "print(\"Sequence IDs: \", seq_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1fa697c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_pos_start = seq_ids.index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "80b34a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility function to find the *last* occurrence of a sequence.\n",
    "def rindex(lst, value):\n",
    "    return len(lst) - operator.indexOf(reversed(lst), value) - 1\n",
    "\n",
    "\n",
    "# Get the end index position (i.e. the last occurrence of 1).\n",
    "context_pos_end = rindex(seq_ids, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9cfaab8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context tokens begin at position 8\n",
      "Context tokens end at position 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Context tokens begin at position\", context_pos_start)\n",
    "print(\"Context tokens end at position\", context_pos_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1bc65",
   "metadata": {},
   "source": [
    "Теперь, когда мы знаем, какие токены являются частью контекста, мы можем посмотреть на их соответствующие сопоставления смещений (offset mappings), чтобы проверить, находятся ли начальная и конечная позиции символов в пределах этих смещений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fb01c0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 5), (6, 10), (11, 13), (14, 17), (18, 19), (19, 23)]\n"
     ]
    }
   ],
   "source": [
    "# These are the corresponding offsets.\n",
    "context_offsets = offset_mapping[context_pos_start : context_pos_end + 1]\n",
    "print(context_offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "95d057ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the lowest offset value lower than or equal to the starting character position?\n",
      "Answer starting character position: 14\n",
      "First offset: (0, 5)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Is the lowest offset value lower than or equal to the starting character position?\"\n",
    ")\n",
    "print(\"Answer starting character position:\", answer_start)\n",
    "print(\"First offset:\", context_offsets[0])\n",
    "\n",
    "# Note how we're checking the first tuple value.\n",
    "print(context_offsets[0][0] <= answer_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "984f36f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the highest offset value higher than or equal to the ending character position?\n",
      "Answer ending character position: 32\n",
      "Last offset: (19, 23)\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"Is the highest offset value higher than or equal to the ending character position?\"\n",
    ")\n",
    "print(\"Answer ending character position:\", answer_end)\n",
    "print(\"Last offset:\", context_offsets[-1])\n",
    "\n",
    "# Note how how we're checking the second tuple value.\n",
    "print(context_offsets[-1][1] >= answer_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d8970",
   "metadata": {},
   "source": [
    "Итак, первая последовательность содержит часть ответа, но полный ответ обрезается. Это подтверждается визуальной проверкой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fe7e684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'Where', ' did', ' Sarah', ' go', '?', '</s>', '</s>', 'Sarah', ' went', ' to', ' The', ' M', 'irth', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.batch_decode(input_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f98419",
   "metadata": {},
   "source": [
    "Сделаем тоже самое для третьей последовательности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e200f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the lowest offset value lower than or equal to the starting character position?\n",
      "Answer starting character position: 14\n",
      "First offset: (11, 13)\n",
      "True\n",
      "Is the highest offset value higher than or equal to the ending character position?\n",
      "Answer ending character position: 32\n",
      "Last offset: (28, 32)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "input_ids = x[\"input_ids\"][2]\n",
    "offset_mapping = x[\"offset_mapping\"][2]\n",
    "seq_ids = x.sequence_ids(2)\n",
    "\n",
    "context_pos_start = seq_ids.index(1)\n",
    "context_pos_end = rindex(seq_ids, 1)\n",
    "\n",
    "context_offsets = offset_mapping[context_pos_start : context_pos_end + 1]\n",
    "\n",
    "print(\n",
    "    \"Is the lowest offset value lower than or equal to the starting character position?\"\n",
    ")\n",
    "print(\"Answer starting character position:\", answer_start)\n",
    "print(\"First offset:\", context_offsets[0])\n",
    "\n",
    "# Note how we're checking the first tuple value.\n",
    "print(context_offsets[0][0] <= answer_start)\n",
    "\n",
    "print(\n",
    "    \"Is the highest offset value higher than or equal to the ending character position?\"\n",
    ")\n",
    "print(\"Answer ending character position:\", answer_end)\n",
    "print(\"Last offset:\", context_offsets[-1])\n",
    "\n",
    "# Note how how we're checking the second tuple value.\n",
    "print(context_offsets[-1][1] >= answer_end)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77df71b",
   "metadata": {},
   "source": [
    "Теперь, когда мы подтвердили, что третья последовательность содержит полный ответ, нам нужно определить, где ответ начинается и заканчивается в *input_ids*. Мы можем сделать это, просканировав offset_mapping слева направо, чтобы найти начало, и справа налево, чтобы найти конец."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3f12713f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = e = 0\n",
    "\n",
    "# Начинаем сканировать offset_mapping слева,\n",
    "# чтобы найти позицию токена, где начинается ответ.\n",
    "# Нет гарантии, что токенизатор выдаст токен, у которого\n",
    "# начальный символ совпадает с первым символом ответа. Когда\n",
    "# это происходит, мы берём позицию предыдущего токена в качестве начала.\n",
    "i = context_pos_start\n",
    "while offset_mapping[i][0] < answer_start:\n",
    "    i += 1\n",
    "if offset_mapping[i][0] == answer_start:\n",
    "    s = i\n",
    "else:\n",
    "    s = i - 1\n",
    "\n",
    "# Поиск конечного токена\n",
    "j = context_pos_end\n",
    "while offset_mapping[j][1] > answer_end:\n",
    "    j -= 1\n",
    "if offset_mapping[j][1] == answer_end:\n",
    "    e = j\n",
    "else:\n",
    "    e = j + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a38ada5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer start token position in context: 9\n",
      "Answer end token position in context: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer start token position in context:\", s)\n",
    "print(\"Answer end token position in context:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0087a9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer lifted from context:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[' The', ' M', 'irth', 'less', ' Cafe']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Answer lifted from context:\")\n",
    "tokenizer.batch_decode(input_ids[s : e + 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f8dd99",
   "metadata": {},
   "source": [
    "Запишем всю логику в функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1020ea8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400\n",
    "stride = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "def prepare_dataset(examples):\n",
    "    # Some tokenizers don't strip spaces. If there happens to be question text\n",
    "    # with excessive spaces, the context may not get encoded at all.\n",
    "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
    "    examples[\"context\"] = [c.lstrip() for c in examples[\"context\"]]\n",
    "\n",
    "    # Tokenize.\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # We'll collect a list of starting positions and ending positions.\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    # Work through every sequence.\n",
    "    for seq_idx in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        seq_ids = tokenized_examples.sequence_ids(seq_idx)\n",
    "        offset_mappings = tokenized_examples[\"offset_mapping\"][seq_idx]\n",
    "\n",
    "        cur_example_idx = tokenized_examples[\"overflow_to_sample_mapping\"][\n",
    "            seq_idx\n",
    "        ]\n",
    "        answer = examples[\"answers\"][cur_example_idx]\n",
    "        answer_text = answer[\"text\"][0]\n",
    "        answer_start = answer[\"answer_start\"][0]\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        context_pos_start = seq_ids.index(1)\n",
    "        context_pos_end = rindex(seq_ids, 1)\n",
    "\n",
    "        s = e = 0\n",
    "        if (\n",
    "            offset_mappings[context_pos_start][0] <= answer_start\n",
    "            and offset_mappings[context_pos_end][1] >= answer_end\n",
    "        ):\n",
    "            i = context_pos_start\n",
    "            while offset_mappings[i][0] < answer_start:\n",
    "                i += 1\n",
    "            if offset_mappings[i][0] == answer_start:\n",
    "                s = i\n",
    "            else:\n",
    "                s = i - 1\n",
    "\n",
    "            j = context_pos_end\n",
    "            while offset_mappings[j][1] > answer_end:\n",
    "                j -= 1\n",
    "            if offset_mappings[j][1] == answer_end:\n",
    "                e = j\n",
    "            else:\n",
    "                e = j + 1\n",
    "\n",
    "        tokenized_examples[\"start_positions\"].append(s)\n",
    "        tokenized_examples[\"end_positions\"].append(e)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f077b0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 400\n",
    "stride = 100\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e4a37729",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = data.map(\n",
    "    prepare_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=data[\"train\"].column_names,\n",
    "    num_proc=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "aed1738c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tokenized_datasets.remove_columns(\n",
    "    [\"offset_mapping\", \"overflow_to_sample_mapping\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "233df361",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = data[\"train\"]\n",
    "val_dataset = data[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a45a240f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    num_workers=4,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=default_data_collator,\n",
    "    num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bbf312",
   "metadata": {},
   "source": [
    "## Дообучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f6739797",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"distilroberta-base\"\n",
    "batch_size = 8\n",
    "lr = 3e-5\n",
    "num_epochs = 1\n",
    "\n",
    "\n",
    "class QAModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n",
    "        self.model.train()\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, **batch):\n",
    "        return self.model(**batch)\n",
    "\n",
    "    def on_train_start(self):\n",
    "        self.model.train()\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        outputs = self(**batch)\n",
    "        loss = outputs.loss\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(self.parameters(), lr=self.lr)\n",
    "        num_training_steps = len(train_dataloader) * num_epochs\n",
    "        lr_scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": lr_scheduler,\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "                \"reduce_on_plateau\": False,\n",
    "                \"monitor\": None,\n",
    "                \"strict\": False,\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f1f1ead8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForQuestionAnswering were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "💡 Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/misha/.pyenv/versions/torch/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:76: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 5070 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/misha/.pyenv/versions/torch/lib/python3.12/site-packages/pytorch_lightning/utilities/model_summary/model_summary.py:231: Precision 16-mixed is not supported by the model summary.  Estimated model size in MB will not be accurate. Using 32 bits instead.\n",
      "\n",
      "  | Name  | Type                        | Params | Mode \n",
      "--------------------------------------------------------------\n",
      "0 | model | RobertaForQuestionAnswering | 81.5 M | train\n",
      "--------------------------------------------------------------\n",
      "81.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "81.5 M    Total params\n",
      "326.117   Total estimated model params size (MB)\n",
      "119       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 2/2761 [00:00<09:36,  4.79it/s, v_num=0, train_loss=6.100]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/misha/.pyenv/versions/torch/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2761/2761 [04:24<00:00, 10.44it/s, v_num=0, train_loss=0.768, val_loss=1.080]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 2761/2761 [04:26<00:00, 10.35it/s, v_num=0, train_loss=0.768, val_loss=1.080]\n"
     ]
    }
   ],
   "source": [
    "model = QAModel(model_name, lr)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=num_epochs,\n",
    "    precision=\"16-mixed\",\n",
    "    gradient_clip_val=1.0,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    ")\n",
    "\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796d7dfd",
   "metadata": {},
   "source": [
    "## Инференс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4183400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== Функция для ответа ==========\n",
    "@torch.inference_mode\n",
    "def get_answer(tokenizer, model, question, context):\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    start_idx = torch.argmax(outputs.start_logits)\n",
    "    end_idx = torch.argmax(outputs.end_logits)\n",
    "    answer_ids = inputs[\"input_ids\"][0, start_idx : end_idx + 1]\n",
    "    return tokenizer.decode(answer_ids, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "907d698c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Mirthless Cafe'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = \"Sarah went to The Mirthless Cafe last night to meet her friend.\"\n",
    "q = \"Where did Sarah go?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ad59c684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' her friend'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"Who did Sarah meet?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "175e4b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' last night'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"When did Sarah meet her friend?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "763ecefa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sarah'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = \"Who went to the restaurant?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62df6f",
   "metadata": {},
   "source": [
    "Но у извлечения ответов из контекста есть свои ограничения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fb4f734",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Задавать логическую загадку сложно, несмотря на то, что\n",
    "# ответ доступен. По правде говоря, здесь есть двусмысленность.\n",
    "q = \"Who did Sarah's friend meet?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5807c0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' to meet her friend'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Модель не может определить, когда на вопрос невозможно\n",
    "# ответить. В некоторых наборах данных для ответов на вопросы\n",
    "# этому явно обучают.\n",
    "q = \"How did Sarah get to the restaurant?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b1e7077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Модель также не является генеративной.\n",
    "q = \"What is a possible reason for why Sarah met her friend?\"\n",
    "get_answer(tokenizer, model, q, c)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
